{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO51pLzeErVAgtGbaF4ZXHc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["ds = load_dataset(\"surrey-nlp/PLOD-CW-25\")\n","\n","label_list = ['O', 'B-AC', 'B-LF', 'I-LF']\n","label_to_id = {label: i for i, label in enumerate(label_list)}\n","\n","for split in [\"train\", \"validation\", \"test\"]:\n","    ds[split] = ds[split].map(lambda x: {\"ner_tags\": [label_to_id[tag] for tag in x[\"ner_tags\"]]})\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","label_all_tokens = True\n","def tokenize_and_align_labels(examples):\n","    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n","\n","    labels = []\n","    for i, label in enumerate(examples[f\"ner_tags\"]):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)\n","        previous_word_idx = None\n","        label_ids = []\n","        for word_idx in word_ids:\n","            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n","            if word_idx is None:\n","                label_ids.append(-100)\n","            # We set the label for the first token of each word.\n","            elif word_idx != previous_word_idx:\n","                label_ids.append(label[word_idx])\n","            else:\n","                label_ids.append(label[word_idx] if label_all_tokens else -100)\n","            previous_word_idx = word_idx\n","\n","        labels.append(label_ids)\n","\n","    tokenized_inputs[\"labels\"] = labels\n","    return tokenized_inputs\n","\n","tokenized_datasets = ds.map(tokenize_and_align_labels, batched=True)\n","model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_list))\n","args = TrainingArguments(\n","    \"bert-based-uncased\",\n","    eval_strategy= \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    logging_strategy=\"steps\",\n","    logging_steps=10,\n",")\n","data_collator = DataCollatorForTokenClassification(tokenizer)\n","\n","def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    # Remove ignored index (special tokens)\n","    true_predictions = [\n","        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    results = metric.compute(predictions=true_predictions, references=true_labels)\n","    return {\n","        \"precision\": results[\"overall_precision\"],\n","        \"recall\": results[\"overall_recall\"],\n","        \"f1\": results[\"overall_f1\"],\n","        \"accuracy\": results[\"overall_accuracy\"],\n","    }\n","\n","trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    processing_class=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","trainer.train()\n","trainer.evaluate()\n","\n"],"metadata":{"id":"gVWnRD65bLuu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n","predictions = np.argmax(predictions, axis=2)\n","\n","true_predictions = [\n","    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","    for prediction, label in zip(predictions, labels)\n","]\n","true_labels = [\n","    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","    for prediction, label in zip(predictions, labels)\n","]\n","\n","results = metric.compute(predictions=true_predictions, references=true_labels)\n","results"],"metadata":{"id":"rNLLCxXfxIhy"},"execution_count":null,"outputs":[]}]}